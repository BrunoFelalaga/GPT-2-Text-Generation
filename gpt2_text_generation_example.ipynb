{"cells":[{"cell_type":"markdown","metadata":{"id":"V9Jncba9DNZL"},"source":["GPT2 is in pytorch_transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"m72NG_bVDNZM"},"outputs":[],"source":["import sys\n","# You may need to play around with the path like I did to find packages\n","!{sys.executable} -m pip install numpy -qU\n","!{sys.executable} -m pip install scipy -qU\n","!{sys.executable} -m pip install pytorch_transformers -qU"]},{"cell_type":"markdown","metadata":{"id":"BjFeFy-qDNZN"},"source":["Import required libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"yy1a0ubDDNZO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738361582938,"user_tz":360,"elapsed":8515,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}},"outputId":"22d2966d-2b8b-4ff6-a649-403f1a753ab3"},"outputs":[{"output_type":"stream","name":"stdout","text":["['/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/IPython/extensions', '/usr/local/lib/python3.11/dist-packages/setuptools/_vendor', '/root/.ipython']\n"]}],"source":["print(sys.path)\n","import torch\n","from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel"]},{"cell_type":"markdown","metadata":{"id":"yjWXBUJkDNZO"},"source":["Load pre-trained model tokenizer (vocabulary)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UTPGlDUKDNZO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738361597255,"user_tz":360,"elapsed":1641,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}},"outputId":"2c3abea1-e7f3-4855-ee29-369b07f65504"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1042301/1042301 [00:00<00:00, 3422974.55B/s]\n","100%|██████████| 456318/456318 [00:00<00:00, 1841407.07B/s]\n"]}],"source":["tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"]},{"cell_type":"markdown","metadata":{"id":"KU4uHxP_DNZO"},"source":["Encode the sentence to complete"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QvNFUbDkDNZP","executionInfo":{"status":"ok","timestamp":1738361599198,"user_tz":360,"elapsed":102,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}}},"outputs":[],"source":["text = \"What mammal has over 40,000 muscles in one body part?\"\n","indexed_tokens = tokenizer.encode(text)"]},{"cell_type":"markdown","metadata":{"id":"GUeQESGqDNZP"},"source":["Convert indexed tokens in a PyTorch tensor"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"BqaGYTeJDNZP","executionInfo":{"status":"ok","timestamp":1738361601243,"user_tz":360,"elapsed":200,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}}},"outputs":[],"source":["tokens_tensor = torch.tensor([indexed_tokens])"]},{"cell_type":"markdown","metadata":{"id":"nq5iOZSoDNZQ"},"source":["Load pre-trained model (weights)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2p1rcF4WDNZQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738361625063,"user_tz":360,"elapsed":22338,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}},"outputId":"7cbf68a6-a890-45b5-912b-d1599c5d41a7"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 665/665 [00:00<00:00, 590059.69B/s]\n","100%|██████████| 548118077/548118077 [00:12<00:00, 42716034.21B/s]\n","/usr/local/lib/python3.11/dist-packages/pytorch_transformers/modeling_utils.py:539: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n"]}],"source":["model = GPT2LMHeadModel.from_pretrained('gpt2')"]},{"cell_type":"markdown","metadata":{"id":"qvW8FYObDNZQ"},"source":["Set the model in evaluation mode to deactivate the DropOut modules"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3e4z38jUDNZQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738361626318,"user_tz":360,"elapsed":96,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}},"outputId":"3946fc58-b377-461c-a772-aa7c44406885"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":7}],"source":["model.eval()"]},{"cell_type":"markdown","metadata":{"id":"XThAWeNVDNZQ"},"source":["If you have a GPU, uncomment the following to put everything on cuda"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3o_t6rjDNZR"},"outputs":[],"source":["# tokens_tensor = tokens_tensor.to('cuda')\n","# model.to('cuda')"]},{"cell_type":"markdown","metadata":{"id":"smgbqhhfDNZR"},"source":["Predict all tokens"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"PyEWfjKwDNZR","executionInfo":{"status":"ok","timestamp":1738361629419,"user_tz":360,"elapsed":429,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}}},"outputs":[],"source":["with torch.no_grad():\n","    outputs = model(tokens_tensor)\n","    predictions = outputs[0]"]},{"cell_type":"markdown","metadata":{"id":"p_nuQtvSDNZR"},"source":["Get the predicted next sub-word"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"LuLBxgxCDNZR","executionInfo":{"status":"ok","timestamp":1738361633056,"user_tz":360,"elapsed":85,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}}},"outputs":[],"source":["predicted_index = torch.argmax(predictions[0, -1, :]).item()\n","predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])"]},{"cell_type":"markdown","metadata":{"id":"ZLU8sNZ1DNZR"},"source":["Print the predicted word"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"llhblvC6DNZR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738361635619,"user_tz":360,"elapsed":218,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}},"outputId":"a2f5097f-1364-43a0-ab72-800fb1acd705"},"outputs":[{"output_type":"stream","name":"stdout","text":[" What mammal has over 40,000 muscles in one body part?\n","\n"]}],"source":["print(predicted_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R78XniFVDNZS"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Examples"],"metadata":{"id":"FZZo83LM7EZr"}},{"cell_type":"code","source":["\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","text = \"the oldest university in Illinois is\"\n","indexed_tokens = tokenizer.encode(text)\n","\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","model.eval()\n","\n","\n","with torch.no_grad():\n","    for token in range(10):  # Generate 10 additional tokens\n","        outputs = model(tokens_tensor)\n","        predictions = outputs[0]\n","\n","        predicted_index = torch.argmax(predictions[0, -1, :]).item()\n","\n","        tokens_tensor = torch.cat((tokens_tensor, torch.tensor([[predicted_index]])), dim=1)\n","\n","predicted_text = tokenizer.decode(tokens_tensor[0].tolist(), skip_special_tokens=True)\n","\n","print(predicted_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"weC9orl03ZfV","executionInfo":{"status":"ok","timestamp":1738361653435,"user_tz":360,"elapsed":10202,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}},"outputId":"a47db512-cd50-4ca0-8fce-4ea33dff37cb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":[" the oldest university in Illinois is now a full-fledged university.\n","\n","The\n"]}]},{"cell_type":"code","source":["tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","text = \"the first president of the USA was President\"\n","indexed_tokens = tokenizer.encode(text)\n","\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","model.eval()\n","\n","\n","with torch.no_grad():\n","    for token in range(3):  # Generate 10 additional tokens\n","        outputs = model(tokens_tensor)\n","        predictions = outputs[0]\n","\n","        predicted_index = torch.argmax(predictions[0, -1, :]).item()\n","\n","        tokens_tensor = torch.cat((tokens_tensor, torch.tensor([[predicted_index]])), dim=1)\n","\n","predicted_text = tokenizer.decode(tokens_tensor[0].tolist(), skip_special_tokens=True)\n","\n","print(predicted_text)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0dSVC5L3ilw","executionInfo":{"status":"ok","timestamp":1738361669217,"user_tz":360,"elapsed":7176,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}},"outputId":"d22735cb-4761-4ccc-99cd-d8be4e95a4b9"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":[" the first president of the USA was President George W.\n"]}]},{"cell_type":"code","source":["tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","text = \"the first president of the USA was President\"\n","indexed_tokens = tokenizer.encode(text)\n","\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","model.eval()\n","\n","\n","with torch.no_grad():\n","    for token in range(10):  # Generate 10 additional tokens\n","        outputs = model(tokens_tensor)\n","        predictions = outputs[0]\n","\n","        predicted_index = torch.argmax(predictions[0, -1, :]).item()\n","\n","        tokens_tensor = torch.cat((tokens_tensor, torch.tensor([[predicted_index]])), dim=1)\n","\n","predicted_text = tokenizer.decode(tokens_tensor[0].tolist(), skip_special_tokens=True)\n","\n","print(predicted_text)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rAP9Eyoe6rBg","executionInfo":{"status":"ok","timestamp":1738361683456,"user_tz":360,"elapsed":9361,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}},"outputId":"f7b59dc5-f8cb-4b1d-b07c-84e5c73476e2"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":[" the first president of the USA was President George W. Bush.\n","\n","The first president\n"]}]},{"cell_type":"markdown","source":["By generating more tokens we can create longer statements.\n","However they seem to not really make sense or seem coherent\n","since each token is chosen statistically and the best fit from a distribution of next tokens. This is seen in the last prompt where 10 tokens are generated for the name of the first USA president.\n","\n","Also the prompts have to be very specific and crafted in a way that leaves no room for ambiguity\n"],"metadata":{"id":"U8OH5hZL6jmb"}},{"cell_type":"code","source":["#  Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# # Navigate to your working directory\n","%cd /content/gdrive/MyDrive/fall_2024_uchicago/generative_ai/wk_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OvZ69kevcKdF","executionInfo":{"status":"ok","timestamp":1738361946940,"user_tz":360,"elapsed":20595,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}},"outputId":"e96cd720-dce4-4606-e265-80d7d1c086cf"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["!touch /content/gdrive/MyDrive/fall_2024_uchicago/generative_ai/wk_1/gpt2_script.py"],"metadata":{"id":"rcbdIB70coKr","executionInfo":{"status":"ok","timestamp":1738362062992,"user_tz":360,"elapsed":275,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["%%writefile /content/gdrive/MyDrive/fall_2024_uchicago/generative_ai/wk_1/gpt2_script.py\n","\n","import torch\n","import sys\n","from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","MAX_LENGTH = 50  # Maximum length of generated text\n","MODEL_NAME = 'gpt2'\n","\n","def generate_text(prompt, max_new_tokens=10):\n","    \"\"\"\n","    Generate text continuation from a prompt using GPT-2\n","\n","    Args:\n","        prompt (str): Input text to continue\n","        max_new_tokens (int): Number of new tokens to generate\n","\n","    Returns:\n","        str: Generated text including prompt\n","    \"\"\"\n","    # Encode prompt\n","    indexed_tokens = tokenizer.encode(prompt)\n","    tokens_tensor = torch.tensor([indexed_tokens]).to(DEVICE)\n","\n","    # Generate tokens\n","    with torch.no_grad():\n","        for _ in range(max_new_tokens):\n","            outputs = model(tokens_tensor)\n","            predictions = outputs[0]\n","            predicted_index = torch.argmax(predictions[0, -1, :]).item()\n","            tokens_tensor = torch.cat((tokens_tensor,\n","                                    torch.tensor([[predicted_index]]).to(DEVICE)),\n","                                    dim=1)\n","\n","    # Decode and return text\n","    return tokenizer.decode(tokens_tensor[0].tolist(), skip_special_tokens=True)\n","\n","# 4. Add example usage function\n","def run_examples():\n","    \"\"\"Run example text generations with different prompts\"\"\"\n","    examples = [\n","        \"What mammal has over 40,000 muscles in one body part?\",\n","        \"the oldest university in Illinois is\",\n","        \"the first president of the USA was President\"\n","    ]\n","\n","    for prompt in examples:\n","        print(f\"\\nPrompt: {prompt}\")\n","        print(f\"Generated: {generate_text(prompt)}\")\n","        print(\"-\" * 50)\n","\n","run_examples()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R2ZkofvIboI4","executionInfo":{"status":"ok","timestamp":1738362156962,"user_tz":360,"elapsed":261,"user":{"displayName":"Bruno Felalaga","userId":"02495120336356568930"}},"outputId":"868e71d6-2738-418c-e2ff-8cf4d428828b"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/gdrive/MyDrive/fall_2024_uchicago/generative_ai/wk_1/gpt2_script.py\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Kw4rpYiubteO"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}